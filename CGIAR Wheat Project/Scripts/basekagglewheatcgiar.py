# -*- coding: utf-8 -*-
"""BasekagglewheatCgiar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ArxfLDM8aDD-Fan-mmatBPfF6ArB_O_C
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries

import pandas as pd
pd.set_option('display.max_columns', None)
from pandas import Series, DataFrame
import numpy as np
import os
from math import sqrt
from sklearn.metrics import mean_squared_error
#import folium
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.metrics import (brier_score_loss, precision_score, recall_score,f1_score)
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import RFE, RFECV
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier,GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import cross_val_score, KFold
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from math import sqrt
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
#import xam

import warnings
warnings.filterwarnings("ignore")

    

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# Imported Libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import Series,DataFrame 
from datetime import date
import datetime as DT
import io
from scipy import stats
from sklearn.metrics import accuracy_score
#import featuretools as ft

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import os

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

from keras.preprocessing.image import load_img, img_to_array

import cv2
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix as CM
from random import randint
from IPython.display import SVG
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plot

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/giara/jukil/Train.csv')
samplesub = pd.read_csv('/content/drive/My Drive/giara/jukil/SampleSubmission.csv')
print(df.shape, df.columns)
df.head()

#image_folder = ['crownV_1', 'earlyV_2', 'midV_3', 'lateV_4', 'floweringV_5', 'anthesisV_6', 'maturityV_7']

def get_images(directory):
    Images = []
    Labels = []  
    label = 0
    
    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.
        if labels == 'crownv1': #Folder contain Glacier Images get the '2' class label.
            label = 0
        elif labels == 'earlyv2':
            label = 1
        elif labels == 'midv3':
            label = 2
        elif labels == 'latev4':
            label = 3
        elif labels == 'floweringv5':
            label = 4
        elif labels == 'anthesisv6':
            label = 5
        elif labels == 'maturityv7':
            label = 6
        
        for image_file in os.listdir(directory+labels): #Extracting the file name of the image from Class Label folder
            image = cv2.imread(directory+labels+r'/'+image_file) #Reading the image (OpenCV)
            image = cv2.resize(image,(100,100)) #Resize the image, Some images are different sizes. (Resizing is very Important)
            Images.append(image)
            Labels.append(label)
        
#     return Images, Labels
    return shuffle(Images,Labels,random_state=817328462) #Shuffle the dataset you just prepared.

def get_classlabel(class_code):
    labels = {0:'crownv1', 1:'earlyv2', 2:'midv3', 3:'latev4', 4:'floweringv5', 5:'anthesisv6', 6:'maturityv7'}
    
    return labels[class_code]



"""Since We are going to do categorical classification, we need to do input image normalization with maximum pixel value 255 and, one_hot encoding with to_categorical function."""

Images, Labels = get_images('/content/drive/My Drive/giara/traindata/') #Extract the training images from the folders.

Images = np.array(Images, dtype=np.float32) #converting the list of images to numpy array.
Images = Images/255.0
Labels = np.array(Labels)

print("Shape of Images:",Images.shape)
print("Shape of Labels:",Labels.shape)

f,ax = plot.subplots(5,5) 
f.subplots_adjust(0,0,3,3)
for i in range(0,5,1):
    for j in range(0,5,1):
        rnd_number = randint(0,len(Images))
        ax[i,j].imshow(Images[rnd_number])
        ax[i,j].set_title(get_classlabel(Labels[rnd_number]))
        ax[i,j].axis('off')

#Let's change digits in the label to categorical vector

#Here is to make a one hot encoded vector from value of label.

from keras.utils import to_categorical
Labels = to_categorical(Labels)

Labels.shape



model = Sequential()
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu', input_shape= (100,100,3)))
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=7, activation='softmax'))

model.summary()



model = tf.keras.models.Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(100, 100, 3)),
    MaxPooling2D(2, 2),
    
    
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    
    
    Conv2D(64, (3, 3), activation='relu'),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    
    
    # Conv2D(128, (3, 3), activation='relu'),
    # Conv2D(128, (3, 3), activation='relu'),
    # MaxPooling2D(2, 2),
    
    # Conv2D(256, (3, 3), activation='relu'),
    # Conv2D(256, (3, 3), activation='relu'),
    # Conv2D(256, (3, 3), activation='relu'),
    # MaxPooling2D(2, 2),
    
    Flatten(),
    Dense(512, activation='relu'),
    Dense(512, activation='relu'),
    Dense(7, activation='softmax')
])

model.summary()

model.compile(optimizer='Adam',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])

history = model.fit(Images, Labels, epochs=25, shuffle=True, validation_split=0.15)

rmse=history.history['root_mean_squared_error']
val_rmse=history.history['val_root_mean_squared_error']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(len(rmse))

fig = plt.figure(figsize=(20,10))
plt.plot(epochs, rmse, 'r', label="Training RMSE")
plt.plot(epochs, val_rmse, 'b', label="Validation RMSE")
plt.xlabel('Epoch')
plt.ylabel('Root_mean_squared_error')
plt.title('Training and validation RMSE')
plt.legend(loc='lower right')
plt.show()
fig.savefig('/content/drive/My Drive/giara/Data/training/RMSE_iter_CNN_4.jpg')

fig2 = plt.figure(figsize=(20,10))
plt.plot(epochs, loss, 'r', label="Training Loss")
plt.plot(epochs, val_loss, 'b', label="Validation Loss")
plt.legend(loc='upper right')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and validation loss')
fig2.savefig('/content/drive/My Drive/giara/Data/training/Loss_CNN_4.jpg')





#PRE-PROCESSING TEST IMAGES

directory = '/content/drive/My Drive/giara/jukil/test/'

test_images = []

for image_file in os.listdir(directory): #Extracting the file name of the image from Class Label folder
            image = cv2.imread(directory+'/'+image_file) #Reading the image (OpenCV)
            image = cv2.resize(image,(100,100)) #Resize the image, Some images are different sizes. (Resizing is very Important)
            test_images.append(image)
          
test_images = np.array(test_images, dtype=np.float32) #converting the list of images to numpy array.
test_images = test_images/255.0
test_images.shape

test_images.shape

#predicting the test set
poll=model.predict_classes(test_images)

#MAKING THE SUBMISSION FILE

freeman = samplesub.copy()
freeman['growth_stage']=poll
freeman['growth_stage'] = freeman['growth_stage'].map({0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7 }, na_action= 'ignore')
freeman.to_csv('cgiar22.csv', index=False)





np.unique(poll)

freeman['growth_stage'].unique()







"""Testing some augumentation techniques for the above used CNN Architecture"""

datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    zoom_range=0.4,
    height_shift_range=0.2,
    horizontal_flip=True)
# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(Images)
# fits the model on batches with real-time data augmentation:
model.fit(datagen.flow(Images, Labels, batch_size=32),
          epochs=30)

Images.shape

