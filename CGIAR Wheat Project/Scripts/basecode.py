# -*- coding: utf-8 -*-
"""Basecode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQtoFRlmxRrdwWcz_8hH440OEKCap8NJ
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries

import pandas as pd
pd.set_option('display.max_columns', None)
from pandas import Series, DataFrame
import numpy as np
import os
from math import sqrt
from sklearn.metrics import mean_squared_error
#import folium
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.metrics import (brier_score_loss, precision_score, recall_score,f1_score)
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import RFE, RFECV
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier,GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import cross_val_score, KFold
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from math import sqrt
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
#import xam

import warnings
warnings.filterwarnings("ignore")

# %cd D:\IPython Jupyter\Analytics Vidya Datasets

    

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# Imported Libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import Series,DataFrame 
from datetime import date
import datetime as DT
import io
from scipy import stats
from sklearn.metrics import accuracy_score
#import featuretools as ft

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import os

print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')



df = pd.read_csv('/content/drive/My Drive/giara/jukil/Train.csv')
samplesub = pd.read_csv('/content/drive/My Drive/giara/jukil/SampleSubmission.csv')
print(df.shape, df.columns)
df.head()

# df=pd.read_csv("Train.csv")
# #test=pd.read_csv("Test.csv")
# samplesubmission=pd.read_csv("SampleSubmission.csv")
# #variabledef=pd.read_csv("VariableDefinitions.txt")

df.head()

# #Changing the label back to letter to eventually convert back to dummies

# df.growth_stage = df.growth_stage.replace(1, "crown")
# df.growth_stage = df.growth_stage.replace(2, "earlyV")
# df.growth_stage = df.growth_stage.replace(3, "midV")
# df.growth_stage = df.growth_stage.replace(4, "lateV")
# df.growth_stage = df.growth_stage.replace(5, "floweringV")
# df.growth_stage = df.growth_stage.replace(6, "anthesisV")
# df.growth_stage = df.growth_stage.replace(7, "maturityV")

# y=pd.get_dummies(df['growth_stage'], drop_first=True) #also stored in a new variable later y
#y2=pd.get_dummies(df['growth_stage'], drop_first=False) #also stored in a new variable later y2 ,just to check for the effect of 'drop_first'

classes_name = [1,2,3,4,5,6,7] #storing names of the classes in the list for future uses.

#Now we define image dimensions
img_height = 100
img_width = 100

X = []

for i in tqdm(range(df.shape[0])):
    path = '/content/drive/My Drive/giara/jukil/train/' + df['UID'][i] + '.jpeg'
    img = image.load_img(path, target_size=(img_height, img_width, 3))
    img = image.img_to_array(img)
    img = img/255.0
    X.append(img)
    
X = np.array(X)

from keras.utils import to_categorical

y = df['growth_stage']

y = y.to_numpy()
#y= to_categorical(y, num_classes=7, dtype='int32')
y.shape



np.unique(y)



# X_train, X_test, y_train, y_test =  train_test_split(X,y, random_state=30, test_size=0.15)

model = Sequential()
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu', input_shape= X[0].shape))
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=7, activation='softmax'))

model.summary()

#BUILDING THE CNN MODEL

# #BUILDING THE CNN MODEL

# model = Sequential()
# model.add(Conv2D(16,(3,3), activation='relu', input_shape= X_train[0].shape))
# model.add(BatchNormalization())
# model.add(MaxPool2D(2,2))
# model.add(Dropout(0.3))

# model.add(Conv2D(32,(3,3), activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPool2D(2,2))
# model.add(Dropout(0.3))

# model.add(Conv2D(64,(3,3), activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPool2D(2,2))
# model.add(Dropout(0.4))

# model.add(Conv2D(128,(3,3), activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPool2D(2,2))
# model.add(Dropout(0.5))

# model.add(Flatten())

# model.add(Dense(128, activation='relu'))
# model.add(BatchNormalization())
# model.add(Dropout(0.5))

# model.add(Dense(128, activation='relu'))
# model.add(BatchNormalization())
# model.add(Dropout(0.5))

# model.add(Dense(units=7, activation='softmax'))


# model.summary()



model.compile(optimizer='Adam',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])

history = model.fit(x=X, y=y, epochs=5, batch_size =100, verbose=1, validation_split = 0.15)



#Plotting Training and validation Accuracy values
epoch_range = range(1,6)
plt.plot(epoch_range, history.history['root_mean_squared_error'])
plt.plot(epoch_range, history.history['val_root_mean_squared_error'])
plt.title('Model_accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','val'], loc='upper left')
plt.show()

#plot training and validation loss values
plt.plot(epoch_range, history.history['loss'])
plt.plot(epoch_range, history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','val'], loc='upper left')
plt.show()



from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

y_pred = model.predict_classes(X_test)

y_pred

np.unique(y_pred)    #check for uniqueness in the predicted result

y_test

np.unique(y_test)    #check for uniqueness in the ytest

mat = confusion_matrix(y_test, y_pred, labels=classes_name)
mat

plot_confusion_matrix(mat, figsize=(9,9), show_normed = True)



#PREDICTING FOR TEST SET

#Now we define image dimensions
img_height = 50
img_width = 50

Xt = []

for i in tqdm(range(samplesub.shape[0])):
    path = '/content/drive/My Drive/giara/jukil/test/' + samplesub['UID'][i] + '.jpeg'
    img = image.load_img(path, target_size=(img_height, img_width, 3))
    img = image.img_to_array(img)
    img = img/255.0
    Xt.append(img)
    
Xt = np.array(Xt)

#PREDICTING THE TEST SET

poll=model.predict_classes(Xt)

#MAKING THE SUBMISSION FILE

freeman = samplesub.copy()
freeman['growth_stage']=poll

freeman.to_csv('cgiar15.csv', index=False)

freeman.head()

