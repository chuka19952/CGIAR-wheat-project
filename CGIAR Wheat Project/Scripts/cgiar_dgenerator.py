# -*- coding: utf-8 -*-
"""cgiar dgenerator

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYuHZSe-zYJPiSIvVVhS1C55dICShRCU
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries

import pandas as pd
pd.set_option('display.max_columns', None)
from pandas import Series, DataFrame
import numpy as np
import os
from math import sqrt
from sklearn.metrics import mean_squared_error
#import folium
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.metrics import (brier_score_loss, precision_score, recall_score,f1_score)
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import RFE, RFECV
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier,GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import cross_val_score, KFold
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from math import sqrt
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
#import xam

import warnings
warnings.filterwarnings("ignore")

# %cd D:\IPython Jupyter\Analytics Vidya Datasets

    

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# Imported Libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.patches as mpatches
import time

# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import collections


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas import Series,DataFrame 
from datetime import date
import datetime as DT
import io
from scipy import stats
from sklearn.metrics import accuracy_score
#import featuretools as ft

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import os

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

from keras.preprocessing.image import load_img, img_to_array

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/giara/jukil/Train.csv')
samplesub = pd.read_csv('/content/drive/My Drive/giara/jukil/SampleSubmission.csv')
print(df.shape, df.columns)
df.head()

# #defining dimensions of images

# img_width=100; img_height=100
# batch_size=32

# TRAINING_DIR = '/content/drive/My Drive/giara/Data/training/'

# train_datagen = ImageDataGenerator(rescale = 1/255.0)


# train_generator = train_datagen.flow_from_directory(TRAINING_DIR,
#                                                     batch_size=batch_size,
#                                                     class_mode='categorical',
#                                                     target_size=(img_height, img_width)
#                                                     )



# VALIDATION_DIR = '/content/drive/My Drive/giara/Data/validation/'

# validation_datagen = ImageDataGenerator(rescale = 1/255.0)


# validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,
#                                                               batch_size=batch_size,
#                                                               class_mode='categorical',
#                                                               target_size=(img_height, img_width)
#                                                              )

# callbacks = EarlyStopping(monitor='mse', patience=5, verbose=1, mode='min')
# best_model_file = '../CNN_best_weights_1.h5'
# best_model = ModelCheckpoint(best_model_file, monitor='root_mean_squared_error', verbose = 1, save_best_only = True)

# model = Sequential()
# model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu', input_shape= (100,100,3)))
# model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu'))
# model.add(BatchNormalization())
# model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))
# model.add(Dropout(0.5))

# model.add(Flatten())
# model.add(Dense(units=128, activation='relu'))
# model.add(Dense(units=7, activation='softmax'))

# model.summary()

# model = tf.keras.models.Sequential([
#     Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
#     MaxPooling2D(2, 2),
    
#     Conv2D(32, (3, 3), activation='relu'),
#     MaxPooling2D(2, 2),
    
#     Conv2D(64, (3, 3), activation='relu'),
#     Conv2D(64, (3, 3), activation='relu'),
#     MaxPooling2D(2, 2),
    
#     Conv2D(128, (3, 3), activation='relu'),
#     Conv2D(128, (3, 3), activation='relu'),
#     MaxPooling2D(2, 2),
    
#     Conv2D(256, (3, 3), activation='relu'),
#     Conv2D(256, (3, 3), activation='relu'),
#     Conv2D(256, (3, 3), activation='relu'),
#     MaxPooling2D(2, 2),
    
#     Flatten(),
#     Dense(512, activation='relu'),
#     Dense(512, activation='relu'),
#     Dense(7, activation='softmax')
# ])

# model.summary()

# model.compile(optimizer='Adam',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])

# history = model.fit_generator(train_generator,
#                               epochs=5,
#                               verbose=1,
#                               validation_data=validation_generator,
#                               callbacks = [best_model, callbacks]
#                               )

# target_dir = '/content/drive/My Drive/giara/Data/training/'
# if not os.path.exists(target_dir):
#   os.mkdir(target_dir)
# model.save(target_dir + 'CNN_model_256.h5')
# model.save_weights(target_dir + 'CNN_weights_256.h5')

# rmse=history.history['root_mean_squared_error']
# val_rmse=history.history['val_root_mean_squared_error']
# loss=history.history['loss']
# val_loss=history.history['val_loss']

# epochs=range(len(rmse))

# fig = plt.figure(figsize=(20,10))
# plt.plot(epochs, rmse, 'r', label="Training RMSE")
# plt.plot(epochs, val_rmse, 'b', label="Validation RMSE")
# plt.xlabel('Epoch')
# plt.ylabel('Root_mean_squared_error')
# plt.title('Training and validation RMSE')
# plt.legend(loc='lower right')
# plt.show()
# fig.savefig('/content/drive/My Drive/giara/Data/training/RMSE_curve_CNN_256.jpg')

# fig2 = plt.figure(figsize=(20,10))
# plt.plot(epochs, loss, 'r', label="Training Loss")
# plt.plot(epochs, val_loss, 'b', label="Validation Loss")
# plt.legend(loc='upper right')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training and validation loss')
# fig2.savefig('/content/drive/My Drive/giara/Data/training/Loss_curve_CNN_256.jpg')





# #Now we define image dimensions AND Convert images to matrix/arrays
# img_height = 100
# img_width = 100

# Xt = []

# for i in tqdm(range(samplesub.shape[0])):
#     path = '/content/drive/My Drive/giara/jukil/test/' + samplesub['UID'][i] + '.jpeg'
#     img = image.load_img(path, target_size=(img_height, img_width, 3))
#     img = image.img_to_array(img)
#     img = img/255.0
#     Xt.append(img)
    
# Xt = np.array(Xt)

# #PREDICTING THE TEST SET
# poll=model.predict_classes(Xt)

# np.unique(poll)    #checking for uniqueness in the predicted values

# #MAKING THE SUBMISSION FILE

# freeman = samplesub.copy()
# freeman['growth_stage']=poll

# freeman.to_csv('cgiar23.csv', index=False)



#JUST A DIFFERENT METHOD OF CARRYING OUT THE PREPROCESSING OF THE TESTSET

#@title
# def preprocess_image(path):
#     img = load_img(path, target_size = (img_height, img_width))
#     a = img_to_array(img)
#     a = np.expand_dims(a, axis = 0)
#     a /= 255.
#     return a


# test_images_dir = '/content/drive/My Drive/giara/jukil/test/'


# test_df = samplesub
# test_df['UID'] = test_df['UID'] + ".jpeg"
# test_dfToList = test_df['UID'].tolist()
# test_ids = [str(item) for item in test_dfToList]



# test_images = [test_images_dir+item for item in test_ids]
# test_preprocessed_images = np.vstack([preprocess_image(fn) for fn in test_images])


# np.save('/content/drive/My Drive/giara/test_preproc_CNN.npy', test_preprocessed_images)



# # load weights into new model
# model= load_model('/content/drive/My Drive/giara/Data/training/CNN_model_256.h5')
# print("Loaded model from disk")

# # evaluate loaded model on test data
# model.compile(loss='mse', optimizer='Adam', metrics=['root_mean_squared_error'])





# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import matplotlib.image as img
# %matplotlib inline
import numpy as np
from collections import defaultdict
import collections
from shutil import copy
from shutil import copytree, rmtree
import tensorflow.keras.backend as K
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import numpy as np
import os
import random
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import regularizers
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.regularizers import l2
from tensorflow import keras
from tensorflow.keras import models
import cv2

import PIL.Image
from PIL.Image import open



n_classes = 7
img_width, img_height = 100, 100
train_data_dir = '/content/drive/My Drive/giara/Data/training/'
validation_data_dir = '/content/drive/My Drive/giara/Data/validation/'
nb_train_samples = 9087
nb_validation_samples = 1608
batch_size = 132

train_datagen = ImageDataGenerator(
    rescale=1. / 255)

validation_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = validation_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')


# inception = InceptionV3(weights='imagenet', include_top=False)
# x = inception.output
# x = GlobalAveragePooling2D()(x)
# x = Dense(128,activation='relu')(x)
# x = Dropout(0.2)(x)

# predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)

# model = Model(inputs=inception.input, outputs=predictions)
# model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])
# checkpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)
# csv_logger = CSVLogger('history_3class.log')

# history = model.fit_generator(train_generator,
#                     steps_per_epoch = nb_train_samples // batch_size,
#                     validation_data=validation_generator,
#                     validation_steps=nb_validation_samples // batch_size,
#                     epochs=30,
#                     verbose=1,
#                     callbacks=[csv_logger, checkpointer])

# model.save('model_trained_3class.hdf5')

model = Sequential()
input= model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu', input_shape= (100,100,3)))
model.add(Conv2D(filters=32,kernel_size=(3,3),padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
predictions= model.add(Dense(units=7, activation='softmax'))

model.summary()

model = model
model.compile(optimizer='sgd',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])
checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/giara/best_model_7class.hdf5', verbose=1, save_best_only=True)
csv_logger = CSVLogger('/content/drive/My Drive/giara/history_7class.log')


history = model.fit_generator(train_generator,
                    steps_per_epoch = nb_train_samples // batch_size,
                    validation_data=validation_generator,
                    validation_steps=nb_validation_samples // batch_size,
                    epochs=6,
                    verbose=1,
                    callbacks=[csv_logger, checkpointer])

model.save('model_trained_7class.hdf5')



class_map_3 = train_generator.class_indices
class_map_3



def plot_rmse(history,title):
    plt.title(title)
    plt.plot(history.history['root_mean_squared_error'])
    plt.plot(history.history['val_root_mean_squared_error'])
    plt.ylabel('root_mean_squared_error')
    plt.xlabel('epoch')
    plt.legend(['train_RMSE', 'validation_RMSE'], loc='best')
    plt.show()
def plot_loss(history,title):
    plt.title(title)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train_loss', 'validation_loss'], loc='best')
    plt.show()

plot_rmse(history,'RMSE Train and Validation')
plot_loss(history,'Loss and Validation loss')



# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Loading the best saved model to make predictions
# #K.clear_session()
# model_best = load_model('/content/drive/My Drive/giara/best_model_7class.hdf5',compile = False)



"""Setting compile=False and clearing the session leads to faster loading of the saved model
    Withouth the above addiitons, model loading was taking more than a minute!
"""

class_list = ['anthesisV_6', 'crownV_1', 'earlyV_2', 'floweringV_5', 'lateV_4', 'maturityV_7', 'midV_3']

#Now we define image dimensions AND Convert images to matrix/arrays
img_height = 100
img_width = 100

Xt = []

for i in tqdm(range(samplesub.shape[0])):
    path = '/content/drive/My Drive/giara/jukil/test/' + samplesub['UID'][i] + '.jpeg'
    img = image.load_img(path, target_size=(img_height, img_width, 3))
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0) 
    img = img/255.0
    Xt.append(img)
    
Xt = np.array(Xt)

pred = model_best.predict_classes(img)
index = np.argmax(pred)
class_list.sort()
pred_value = class_list[index]

